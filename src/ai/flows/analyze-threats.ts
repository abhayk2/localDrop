// This is an autogenerated file from Firebase Studio.
'use server';

/**
 * @fileOverview A security threat analysis AI agent for uploaded files.
 *
 * - analyzeThreats - A function that handles the threat analysis process.
 * - AnalyzeThreatsInput - The input type for the analyzeThreats function.
 * - AnalyzeThreatsOutput - The return type for the analyzeThreats function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const AnalyzeThreatsInputSchema = z.object({
  fileDataUri: z
    .string()
    .describe(
      "The file to be analyzed, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
  filename: z.string().describe('The name of the file being analyzed.'),
});
export type AnalyzeThreatsInput = z.infer<typeof AnalyzeThreatsInputSchema>;

const AnalyzeThreatsOutputSchema = z.object({
  hasThreats: z.boolean().describe('Whether or not the file contains threats.'),
  threatReport: z.string().describe('A report of the threats found in the file.'),
});
export type AnalyzeThreatsOutput = z.infer<typeof AnalyzeThreatsOutputSchema>;

export async function analyzeThreats(input: AnalyzeThreatsInput): Promise<AnalyzeThreatsOutput> {
  return analyzeThreatsFlow(input);
}

const prompt = ai.definePrompt({
  name: 'analyzeThreatsPrompt',
  input: {schema: AnalyzeThreatsInputSchema},
  output: {schema: AnalyzeThreatsOutputSchema},
  prompt: `You are a security expert specializing in analyzing files for potential security threats.

You will use the file contents to determine if the file is safe or not. You will output a threat report, and set the hasThreats output field appropriately.

Filename: {{{filename}}}
File Contents: {{media url=fileDataUri}}`,
});

const analyzeThreatsFlow = ai.defineFlow(
  {
    name: 'analyzeThreatsFlow',
    inputSchema: AnalyzeThreatsInputSchema,
    outputSchema: AnalyzeThreatsOutputSchema,
  },
  async input => {
    const {output} = await prompt(input);
    return output!;
  }
);
